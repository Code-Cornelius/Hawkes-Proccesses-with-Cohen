\documentclass[11pt]{book}
 
\newcommand{\reporttitle}{Introduction to Multi-Dimensional Hawkes' Self-Exciting Point Processes: Simulation, Estimation, Adaptative Kernels and Changepoint Analysis.}
\newcommand{\reportauthor}{Niels D. C. CARIOU KOTLAREK}
\newcommand{\course}{Summer Project}
\newcommand{\professor}{Ed COHEN}
% include file with configuration.
\input{chap/configuration} % various packages needed for maths etc.
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{lscape}
\usepackage{rotating}


\newcommandx{\willlastcheck}[1]{\todo[linecolor=SpringGreen,backgroundcolor=SpringGreen!25,bordercolor=SpringGreen,inline]{#1}}
\newcommandx{\willmuchlater}[1]{\todo[linecolor=Plum,backgroundcolor=Plum!25,bordercolor=Plum,inline]{#1}}


\hbadness = 15000
\hfuzz = 100 pt % do not trigger any warning related to hbox. I.E. that the line is too long. I had plenty of such useless warnings. 
\vbadness=\maxdimen












% specific to this document:
\newcommand{\lsum}[1]{\sum_{ \{k : t_k^#1 < T \} }}
\newcommand{\lexp}[1]{
\exp \left ( - \beta_{m,n} \cdot ( T - t_k^#1 ) \right ) 
}
\newcommand{\denomR}{\nu_m + \sum_{j=1}^p \alpha_{m,j} R_{m,j} (k) }

\newcommand{\sequence}[1]{\{ #1 \}_{ i \in \N} }
\newcommand{\sequencetime}{\{t\}_{t \in \mathcal I} }


\begin{document}
\frontmatter
\pagestyle{front}


\listoftodos[LIST OF TODOS]
\newpage



\input{chap/page_de_garde}
\thispagestyle{empty}%pour la page de garde toute blanche

\input{chap/resume}                                                                
\input{chap/merci}





\shorttableofcontents{Content}{0}%sommaire avec uniquement les chapitres
\addcontentsline{toc}{chapter}{Content}%ajout du sommaire dans le sommaire!

\mainmatter
\pagestyle{main}


\input{chap/chap1}
\input{chap/chap2}
\input{chap/chap3}







\chapter{Numerical Results of HATDEP}

\section{Presentation}
This section is for presenting some empirical observations à propos the new adaptive algorithm using the new introduced function $g$ defined in theorem \ref{thrm:new_g}. We first precise our settings, then the results and we try to give some meaningful observations and interpretations. It yields some critical ideas about how to chose the parameters. We first show how the HAPDEP improves estimation in the case of windows that are too big, and secondly how the algorithm isn't harming estimation that are already good.

\section{Considered Evolution for the Parameters}
\begin{figure}
\centering
\subfloat{{
\includegraphics[width = 0.32 \textwidth]{imag/chap3/EVOL_PARAM/Figure_1.png}
}}
\subfloat{{
\includegraphics[width = 0.32 \textwidth]{imag/chap3/EVOL_PARAM/Figure_2.png}
}}\\
\subfloat{{
\includegraphics[width = 0.32 \textwidth]{imag/chap3/EVOL_PARAM/Figure_3.png}
}} 
\subfloat{{
\includegraphics[width = 0.32 \textwidth]{imag/chap3/EVOL_PARAM/Figure_4.png}
}}\\
\subfloat{{
\includegraphics[width = 0.32 \textwidth]{imag/chap3/EVOL_PARAM/Figure_5.png}
}}
\caption{Plots of the different evolutions we consider.}
\label{fig:evol_functions}
\end{figure}

We are going to consider a few evolution functions for the parameters. We want to see how different functions influences the results. We chose constant parameters, linear evolution, one-jump evolution (piece-wise constant), linear evolution combined with one-jump evolution and finally a sinusoidal evolution followed by a constant value. In the three last, there is a breakpoint in the function: either a jump, or the stop of a periodic behaviour. This choice is motivated for change-point analysis in 
chapter \ref{chap:change_point}. We observe the basic functions in figure \ref{fig:evol_functions}.

Essentially, in the univariate case, we plot in fig. \ref{fig:evol_functions_interact} how the parameters evolve with respect to each other. Those coefficients are the one we use for the simulations. We observe the process over an interval $[0,T]$ which corresponds to roughly 7500 jumps. This number of event is quite important, and allows us to observe almost asymptotic behaviour for our estimation. At the same time, such high number of jumps is not making the simulation too long for them to not be doable. We estimated for each of the 50 points on [0.05*T,0.95 * T]\footnote{We chosed to skip the 5$\%$ at the boundaries of the simulation. That way, we hope to minimize the effect of boundaries on the estimation.}  50 times; hence each plot represents the conclusion of 2500 simulations. The data is available in CSV files available on the github of the project (cf. in appendix section \ref{csv-files}).

\begin{figure}
\centering
\subfloat{{
\includegraphics[width = 0.48 \textwidth]{imag/chap3/EVOL_PARAM/AFigure_1.png}
}}
\subfloat{{
\includegraphics[width = 0.48 \textwidth]{imag/chap3/EVOL_PARAM/AFigure_2.png}
}}\\
\subfloat{{
\includegraphics[width = 0.48 \textwidth]{imag/chap3/EVOL_PARAM/AFigure_3.png}
}} 
\subfloat{{
\includegraphics[width = 0.48 \textwidth]{imag/chap3/EVOL_PARAM/AFigure_4.png}
}}\\
\subfloat{{
\includegraphics[width = 0.55 \textwidth]{imag/chap3/EVOL_PARAM/AFigure_5.png}
}}
\caption{Plots of the three parameters of a Hawkes process. Red is $\beta$, purple is $\alpha$ and the blue line represents the evolution of $\nu$. The 5 shapes are the one we consider and are related to the functions ploted on fig. \ref{fig:evol_functions}.}
\label{fig:evol_functions_interact}
\end{figure}













\section{HATDAP for improving performances}
\label{section_first_simul}
As a first array of trials, we consider our recommanded parameters: $L$ and $R$ defined respectively as being the $4$ and $96$ quantiles; $h$ being $2.5$. We chose $l$ as a function of the first width. The reason for that is that if one takes a width smaller than the total length, the kernel is not covering the whole observation window and so it is not consistant, in the sense that the criterea is global but the kernel stays local. For that reason, a good idea would be to adapt the value $l$ to the value of the original kernel. We recommand to use the width of the arbitrary first width divided by 2. That way, one has a kernel considering all the points over the observation line, independently of its center.



Also, we notice a small bump in the first estimations. It is due to the value of $l$ being too big: a large kernel is suppose to appear for values close to the mean, however, for such values, 


We are also going to use $1/4$ of the observation length as being the width for the kernels of the first estimate (hence $l = 1/8$), following the recommandation from section \ref{subsection:optimal_width} of using an arbitrary kernel width. This width is quite big relatively to the total number of jumps, and we could have used a smaller one. However, we want to observe the behaviour under non perfect condition of the adaptive algorithm.



\subsection{Constant Parameters}
The figures are drawn in apendix as: \ref{fig:first_estimate_0_alpha}, \ref{fig:first_estimate_0_beta}, \ref{fig:first_estimate_0_nu}. 


We previously mentionned how the scaling is interfering with rescaling the kernels for constant parameters. The scaling is then more focused on the noise than anything relevant about the data (cf subsection \ref{subsection:scaling}). We draw on the fig. \ref{fig:impact_g_flat} some estimation for one parameter (based on the same data as for \ref{fig:first_estimate_0_nu}), and we observe how it impacts the kernel adaptive process. The evolution is hectic and there is definetely no clear pattern. In such case, one should increase the width of the kernels to the maximum and weight all the data in the same manner. Basically, the first estimate parametrize the second as being a constant estimation. 

\begin{figure}
\centering
\includegraphics[width = 0.90 \textwidth]{../imag/chap3/IMPACT_G/flat_all_kernels.png}
\caption{TO WRITE.}
\label{fig:impact_g_flat}
\end{figure}


We observe an expected improvement. It isn't directly induced by $g$, but by the scaling tolerance we introduced. It deals very well with such trends. The impact of the scale on the kernels is that they became totally flat: one should be careful about the scale on the right.

\textbf{Conclusion:} Constant parameters should be treated differently than the rest. We deal with it with a tolerance threshold.




\subsection{Linear Growth}
The figures are drawn in apendix as: \ref{fig:first_estimate_1_alpha}, \ref{fig:first_estimate_1_beta}, \ref{fig:first_estimate_1_nu}.

Let's first give a look to how the kernels are evolving with the first estimate. On fig. \ref{fig:impact_g_linear}, we see the expected behaviour for the evolution of the kernels. Extreme values are estimated with smaller kernels, and values close to the mean with wider kernels\footnote{They are wide and it makes sense. Both sides compensate each other, the parameters in the middle appear like being the average of the whole observed period.}. At the core, it is the behaviour one could expect.

\begin{figure}
\centering
\includegraphics[width = 0.90 \textwidth]{../imag/chap3/IMPACT_G/linear_growth_all_kernels.png}
\caption{TO WRITE.}
\label{fig:impact_g_linear}
\end{figure}

It does show some improvements, since the estimate looks overall flatter. Then, the function $g$ works great for linear trends. The points where the kernels are the smallest show signs of more variability. This is due to having a parameter $h$ too big for this case scenario / an original kernel width too small.


\textbf{Conclusion:} Hawkes Adaptive Time Dependant Estimation of Parameters can be tried with a few different first trials, in the exact same way as KDE.




\subsection{Simple Jump}
The figures are drawn in apendix as: \ref{fig:first_estimate_2_alpha}, \ref{fig:first_estimate_2_beta}, \ref{fig:first_estimate_2_nu}.

We observe a transition phase in between the two stationary phases. It is roughly 1000 long, which is the length of the kernel. Over the two phases, we observe small kernels, and during the jump bigger kernels. 


\willlastcheck{ paragraph not useful later? }
A last comment, there is a spike in beta's estimation, that is simply due to an overwhelmingly high estimate and to an effect of batch-mean, that can happen due to the estimation used method. It is totally random, and should be considered as extreme noise. For that reason, we excluded the beta dimension from the scaling algorithm.


\begin{figure}
\centering
\includegraphics[width = 0.90 \textwidth]{../imag/chap3/IMPACT_G/jump_all_kernels.png}
\caption{TO WRITE.}
\label{fig:impact_g_jump}
\end{figure}


\textbf{Conclusion:}





\subsection{Linear Growth and Jump}
The figures are drawn in apendix as: \ref{fig:first_estimate_3_alpha}, \ref{fig:first_estimate_3_beta}, \ref{fig:first_estimate_3_nu}.

The first estimate struggles to capture accuratly the jump in the parameters' function. There is a transition phase between the two behaviours which could be shortened. For parameter $\alpha$, it last for 1000, for parameter $\nu$ roughly 800. It corresponds to the size of the kernel.

We see that the jump is way better estimated after HATDEP. The transition reduces to around 600 but more importantly, one clearly distinguishes the two phases. Also, we observe that the adaptive algorithm did increase the phenomena appearing in the first graph: alpha was estimated close to the jump (and after the adaptive process, even closer) while for nu, the jump was detected too early, and it increased with the adaptive. So the algorithm is pinpointing some underlying phenomenas, as expected.

Another thing to consider is that the jump isn't perfectly located, but it is a dimensionality problem. The algorithm isn't adapting with respect to one parameter, but with respect to all the parameters. Since the jumps for alpha and nu are at different times, it isn't clear how to adapt in the best fashion. Here, the algorithm has increased the kernels for all points in between the two jumps, making the left side of the jump for alpha as well as the right side of the jump for nu very precise. Overall, the two jumps are better localized, and the function is sharper (reflecting the non continuous behaviour of it).

\textbf{Conclusion:} The algorithm is limited to low dimensional problems, like 2 or 3 dimensions maximum.

\subsection{Sinusoïdal Growth}
The figures are drawn in apendix as: \ref{fig:first_estimate_4_alpha}, \ref{fig:first_estimate_4_beta}, \ref{fig:first_estimate_4_nu}.

We change $L$ for a bigger value, because we expect to have many small values induced by the plateau ending phase. We take $L=10\%$, and $R$ reduced to $98\%$.

The first estimate struggles to capture the variability of the sinus. We have blatantly used a too wide kernel. However, it is interesting to see how the algorithm would react to it.

We observe on our test image \ref{fig:impact_g_sin} that high peak values have narrow kernels and that the stationary phase after 6000 has the low narrow kernels. Then we expect the high points to be precise but the fact that no small kernels are left for the low points of the sinus might be an issue for a precise estimation.



Now we observe that the changing point at 6000 is better captured. However, on the other hand, since we scale the kernel accordingly, it also means the lower parts of the sinus are missed, while the upper parts aren't. We would have observed a different behaviour if the final value of the parameters would have centred with respect to the sinus. In order to compensate for that, one should increase the number of kernels for low-extremal values (the parameter 	L), as well as used smaller kernels from the beginning; that should take into account the variability of the function.

\textbf{Conclusion:} Depending on the expected behaviour of the function, whether there are many low points or many high points, one should adapt the quantiles $L$ and $R$.


\begin{figure}
\centering
\includegraphics[width = 0.90 \textwidth]{../imag/chap3/IMPACT_G/sin_all_kernels.png}
\caption{TO WRITE.}
\label{fig:impact_g_sin}
\end{figure}




\section{Better First Guess}
\label{section_second_simul}
We now use a smaller kernel from the beginning. We are studying simply the one jump case, the linear growth and jump case and the sinusoïdal curve. For them, we use $1/5$ for all of them apart from $1/8$ for the sinusoïdal curve. 

\subsection{Simple Jump}

The figures are drawn in apendix as: \ref{fig:second_estimate_2_alpha}, \ref{fig:second_estimate_2_beta}, \ref{fig:second_estimate_2_nu}.


\subsection{Linear growth and Jump}


The figures are drawn in apendix as: \ref{fig:second_estimate_3_alpha}, \ref{fig:second_estimate_3_beta}, \ref{fig:second_estimate_3_nu}.


\subsection{Sinusoïdal Growth}


The figures are drawn in apendix as: \ref{fig:second_estimate_4_alpha}, \ref{fig:second_estimate_4_beta}, \ref{fig:second_estimate_4_nu}.


\section{Conclusion About the Algorithm}

It doesn't harm much the estimation. Of course a bit, then one can reduce the number $h$.

problem of dimensionality. 

problem of multiple shape: better for one time jumps. If all dimensions can have similar behaviour.

































\section{Other Fancy Methods}
\subsection{Radial Basis Function}

I won't have time for that. There was the topic of radial basis function as well as the truncated kernels I saw in Wand. 


\willmuchlater{Need to read about method. \newline and write about it. \newline. Perhaps I wont have the time.}

\willmuchlater{ I also want to look at the truncated kernels. Notamment, sait-on si il y a un jump ou non?}






\input{chap/chap4}









































\cleardoublepage% le corps du document est terminé

\appendix
\pagestyle{back}




\part{Appendix}
\input{chap/annexe}
\input{chap/annexe_images}
\input{chap/annexe_code}


\backmatter

\printbibliography
\nocite{*}

\addcontentsline{toc}{chapter}{References}

%\listoffigures
%\addcontentsline{toc}{chapter}{Table des figures}

\tableofcontents%table des matières plus complète
\addcontentsline{toc}{chapter}{Table of content}%ajout de la table des matières dans la table des matières !



\end{document}
